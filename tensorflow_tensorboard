# -*- coding: utf-8 -*-
"""
Created on Wed Sep 19 20:28:04 2018
logistic 逻辑回归训练，谷哥手写字识别
数字可视化，tensorboard 图形可视界面
@author: Administrator
"""

import tensorflow as tf
import numpy as np
import matplotlib as pls
from tensorflow.examples.tutorials.mnist import input_data

def variable_sumaries(i):
    with tf.name_scope('sumaries'):
        mean=tf.reduce_mean(i)
        tf.summary.scalar('mean',mean)
        with tf.name_scope('stddev'):
            stddev=tf.sqrt(tf.reduce_mean(tf.square(i-mean)))
        tf.summary.scalar('stddev',stddev)
        tf.summary.scalar('max',tf.reduce_max(i))
        tf.summary.scalar('min',tf.reduce_min(i))
        tf.summary.histogram('histogram',i)        
#你可以把MNIST_data放在程序所在的文件夹里面，就会自己找到。
mnist=input_data.read_data_sets("MNIST_data",one_hot=True)
edata=100
ndata=mnist.train.num_examples//edata

with tf.name_scope('input'):
    x=tf.placeholder(tf.float32,[None,784],name='x-input')
    y=tf.placeholder(tf.float32,[None,10],name='y-input')

with tf.name_scope('layer'):
    with tf.name_scope('wights1'):
        w1=tf.Variable(tf.truncated_normal([784,100],stddev=0.1))
        variable_sumaries(w1)
    with tf.name_scope('biaes1'):    
        b1=tf.Variable(tf.ones([100])+0.01)
        variable_sumaries(b1)
    with tf.name_scope('a1'):
        a1=tf.nn.tanh(tf.matmul(x,w1)+b1)

    with tf.name_scope('wights2'):
        w2=tf.Variable(tf.truncated_normal([100,100],stddev=0.1))
    with tf.name_scope('biaes2'):   
        b2=tf.Variable(tf.ones([100])+0.01)
    with tf.name_scope('a2'):        
        a2=tf.nn.tanh(tf.matmul(a1,w2)+b2)
        
    with tf.name_scope('wights3'):   
        w3=tf.Variable(tf.truncated_normal([100,100],stddev=0.1))
    with tf.name_scope('biaes3'):   
        b3=tf.Variable(tf.ones([100])+0.01)
    with tf.name_scope('a3'):
        a3=tf.nn.tanh(tf.matmul(a2,w3)+b3)
        
    with tf.name_scope('wights4'):   
        w4=tf.Variable(tf.truncated_normal([100,100],stddev=0.1))
    with tf.name_scope('biaes4'):   
        b4=tf.Variable(tf.ones([100])+0.01)
    with tf.name_scope('a4'):       
        a4=tf.nn.tanh(tf.matmul(a3,w4)+b4)
        
    with tf.name_scope('wights5'):  
        w5=tf.Variable(tf.truncated_normal([100,10],stddev=0.1))
    with tf.name_scope('biaes5'):  
        b5=tf.Variable(tf.ones([10])+0.01)
    with tf.name_scope('a5'):       
        a5=tf.nn.tanh(tf.matmul(a4,w5)+b5)

#loss=tf.reduce_mean(tf.square(a3-y))
with tf.name_scope('loss'):
    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=a5))
    tf.summary.scalar('loss',loss)
with tf.name_scope('train'):    
    train_step=tf.train.GradientDescentOptimizer(0.4).minimize(loss)

#acc1=tf.equal(tf.argmax(y,1),tf.argmax(a2,1))
#accuracy=tf.reduce_mean(tf.cast(acc1,tf.float32))
with tf.name_scope('accuracy'):
    with tf.name_scope('correct_prediction'):
        acc1=tf.equal(tf.argmax(y,1),tf.argmax(a5,1))
#        tf.summary.scalar(acc1)
    with tf.name_scope('correct_prediction'):

        accuracy=tf.reduce_mean(tf.cast(acc1,tf.float32))
        tf.summary.scalar('accuracy',accuracy)
merged=tf.summary.merge_all()        
init=tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    
    writer=tf.summary.FileWriter('logs/',sess.graph)
    for i in range(50):
        for ii in range(ndata):
            x_d,y_d=mnist.train.next_batch(edata)
            summary,_ =sess.run([merged,train_step],feed_dict={x:x_d,y:y_d})
#        write.add_summary(summary,i)
        writer.add_summary(summary,i)
        acc=sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})   
        print(i+1,':',acc)          
    
